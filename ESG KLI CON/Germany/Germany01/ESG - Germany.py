import os.path
import pandas as pd
import requests
from bs4 import BeautifulSoup, XMLParsedAsHTMLWarning
import time
from datetime import datetime
import re
from urllib.parse import urljoin
from deep_translator import GoogleTranslator
import warnings

warnings.filterwarnings("ignore", category=XMLParsedAsHTMLWarning)

def format_date(date_str):
    try:
        return datetime.strptime(date_str, "%d.%m.%Y").strftime("%Y-%m-%d")
    except:
        return ""

word_list = {
    "Protokoll": "Protocol",
    "Regeln": "Regulation",
    "Verordnung": "Ordinance",
    "√úbereinkommen": "Convention",
    "Gesetz": "Law",
    "Rechtsnormen": "Legal Norms"
}
allowed_types = list(word_list.keys())

base_url = "https://www.gesetze-im-internet.de/"
search_url = "https://www.gesetze-im-internet.de/cgi-bin/htsearch"
headers = {
    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.6',
    'Content-Type': 'application/x-www-form-urlencoded',
    'Origin': 'https://www.gesetze-im-internet.de',
    'Referer': 'https://www.gesetze-im-internet.de/titelsuche.html'
}
session = requests.Session()

out_excel_file = os.path.join(os.getcwd(), "Germany.xlsx")

with open('keywords.txt', 'r', encoding='utf-8') as file:
    keywords = [line.strip() for line in file if line.strip()]

all_results = []
visited_urls = set()

for keyword in keywords:
    print(f"\nüîé Processing keyword: {keyword}")
    try:
        page = 1
        while True:
            data = {
                'config': 'Titel_bmjhome2005',
                'method': 'and',
                'words': keyword,
                'page': str(page),
                'suche': 'Suchen'
            }

            response = session.post(search_url, data=data, headers=headers)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'lxml')

            strong_tags = soup.find_all('strong')
            result_urls = []
            for strong in strong_tags:
                links = strong.find_all('a', href=True)
                for link in links:
                    href = link.get('href')
                    if href:
                        full_url = urljoin(base_url, href)
                        if full_url not in visited_urls:
                            result_urls.append(full_url)
                            visited_urls.add(full_url)

            if not result_urls:
                print(f"No results found on page {page} for keyword: {keyword}")
                break

            for url in result_urls:
                print(f"   üåê Processing URL: {url}")
                try:
                    res_page = session.get(url, headers=headers)
                    res_page.raise_for_status()
                    soup_page = BeautifulSoup(res_page.text, 'lxml')

                    html_link_tag = soup_page.find('a', string='HTML')
                    if not html_link_tag or not html_link_tag.get('href'):
                        print(f"      ‚ö†Ô∏è No HTML version link found for {url}")
                        continue

                    html_relative_url = html_link_tag['href']
                    full_html_url = urljoin(url, html_relative_url)

                    html_response = session.get(full_html_url, headers=headers)
                    html_response.raise_for_status()
                    html_soup = BeautifulSoup(html_response.text, 'lxml')

                    h1_tag = html_soup.find('h1')
                    if h1_tag:
                        span = h1_tag.find('span', class_='jnlangue')
                        title = span.text.strip() if span else h1_tag.text.strip()
                    else:
                        title = ''

                    reg_type = ''
                    if title:
                        for t in allowed_types:
                            if t in title:
                                reg_type = t
                                break
                        if not reg_type:
                            print(f"      ‚è≠Ô∏è Skipping '{title}' (not an allowed regulation type).")
                            continue

                    reg_type_english = word_list.get(reg_type, reg_type)

                    date_of_adoption = ''
                    match_adopt = re.search(r'Ausfertigungsdatum:\s*(\d{2}\.\d{2}\.\d{4})', html_response.text)
                    if match_adopt:
                        date_of_adoption = format_date(match_adopt.group(1))

                    entry_force_date = ''
                    match_force = re.search(r'mWv\s*(\d{2}\.\d{2}\.\d{4})', html_response.text)
                    if match_force:
                        entry_force_date = format_date(match_force.group(1))

                    translated_title = GoogleTranslator(source='de', target='en').translate(title)
                    time.sleep(1)

                    metadata = {
                        'Jurisdiction': 'Germany',
                        'Original Title': title,
                        "English Translation": translated_title,
                        'Type of Regulation': reg_type_english,
                        'Source': full_html_url,
                        'Date of Adoption': date_of_adoption,
                        'Entry into Force Date': entry_force_date,
                    }

                    all_results.append(metadata)
                    print(metadata)
                    print(f"      ‚úÖ Saved: {title}")
                    time.sleep(1)

                except Exception as e:
                    print(f"      ‚ùå Error processing URL {url}: {str(e)}")

            next_page = soup.find('a', href=True, string=str(page + 1))
            if next_page:
                page += 1
                print(f"‚û°Ô∏è Moving to page {page} for keyword: {keyword}")
                continue
            else:
                break

    except Exception as e:
        print(f"Error processing keyword '{keyword}': {str(e)}")

df = pd.DataFrame(all_results)
df.to_excel(out_excel_file, index=False)

print(f"\nüéâ Scraping completed. Results saved to {out_excel_file}")
